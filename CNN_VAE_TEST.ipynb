{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "import CNN_VAE\n",
    "import importlib\n",
    "import numpy as np\n",
    "# Reload the module to ensure changes are recognized\n",
    "importlib.reload(CNN_VAE)\n",
    "\n",
    "# Import the updated class and custom dataset\n",
    "from CNN_VAE import ConvolVariatinalAutoEncoder, CustomImageDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model parameters\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "Z_DIMS = [8]\n",
    "H_DIMS = [32]\n",
    "im_size =(256, 256)\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(im_size),  # Resize images to 128x128\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomImageDataset(root_dir='../dataset/Waves4', transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Move the images to the same device as the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with z_dim=8 and h_dim=32...\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n",
      "1.0383261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for h_dim in H_DIMS:\n",
    "    for z_dim in Z_DIMS:\n",
    "            \n",
    "        print(f'Training model with z_dim={z_dim} and h_dim={h_dim}...')\n",
    "        # Create an instance of the model\n",
    "        model = ConvolVariatinalAutoEncoder(input_dim=1, h_dim=h_dim, z_dim=z_dim, output_channels=1, im_size=im_size)\n",
    "\n",
    "        # Load the saved state dictionary\n",
    "        model.load_state_dict(torch.load('./Modelos_VAE/cnn_vae_model_Z{}_H{}_epoch_6000.pth'.format(z_dim, h_dim)))\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Pass the images through the model\n",
    "        for x, _ in train_loader:  \n",
    "            x = x.to(device)\n",
    "            \n",
    "            x_reconstructed, mu, logvar = model(x)\n",
    "            # Move the reconstructed images back to CPU for saving\n",
    "            x_reconstructed = x_reconstructed.cpu().detach().numpy()\n",
    "            error = x_reconstructed-x.cpu().detach().numpy()\n",
    "            # Transpose the image to (height, width, channels) format for saving\n",
    "            for j in range(x_reconstructed.shape[0]):\n",
    "                print(x_reconstructed.max())\n",
    "                image = x_reconstructed[j].transpose(1, 2, 0)\n",
    "                error_image = error[j].transpose(1, 2, 0)\n",
    "                # Convert the image to a NumPy array\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "                error_image = (error_image * 255).astype(np.uint8)\n",
    "                # Convert the NumPy array to a PIL image\n",
    "                \n",
    "                image_pil = Image.fromarray(np.squeeze(image))\n",
    "                error_image = Image.fromarray(np.squeeze(error_image))\n",
    "                # Save the image as a PNG file\n",
    "                try: \n",
    "                    os.mkdir('Modelos_VAE/Imagens_sample/Model_Z{}_H{}'.format(z_dim, h_dim))\n",
    "                    os.mkdir('Modelos_VAE/Error_sample/Model_Z{}_H{}'.format(z_dim, h_dim))\n",
    "                    \n",
    "                except: pass\n",
    "                image_pil.save('Modelos_VAE/Imagens_sample/Model_Z{}_H{}/Im{}.png'.format(z_dim, h_dim, j))\n",
    "                error_image.save('Modelos_VAE/Error_sample/Model_Z{}_H{}/Im{}.png'.format(z_dim, h_dim, j))\n",
    "                \n",
    "# # Select the first image in the batch\n",
    "# image = x_reconstructed[1]\n",
    "\n",
    "# # Transpose the image to (height, width, channels) format for saving\n",
    "# image = image.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# # Convert the image to a NumPy array\n",
    "# image = (image * 255).astype(np.uint8)\n",
    "\n",
    "# # Convert the NumPy array to a PIL image\n",
    "# image_pil = Image.fromarray(image)\n",
    "\n",
    "# # Save the image as a PNG file\n",
    "# image_pil.save('image_2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0292159"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reconstructed.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pil = Image.fromarray(np.squeeze(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99, 101, 100, ...,  89,  87,  85],\n",
       "       [ 99, 101, 102, ...,  88,  86,  84],\n",
       "       [ 98, 101,  99, ...,  89,  87,  86],\n",
       "       ...,\n",
       "       [ 80,  83,  77, ...,  97,  94,  91],\n",
       "       [ 79,  80,  80, ...,  91,  98,  79],\n",
       "       [ 81,  83,  84, ...,  92,  93,  65]], dtype=uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
